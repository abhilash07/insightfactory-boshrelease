<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration xmlns:xi="http://www.w3.org/2001/XInclude">

<!-- i/o properties -->

  <property>
    <name>io.file.buffer.size</name>
    <value>131072</value>
    <description>
        The size of buffer for use in sequence files. The size of this
        buffer should probably be a multiple of hardware page size
        (4096 on Intel x86), and it determines how much data is
        buffered during read and write operations.
    </description>
  </property>

  <property>
    <name>io.serializations</name>
    <value>org.apache.hadoop.io.serializer.WritableSerialization</value>
  </property>

  <property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec</value>
    <description>
        A list of the compression codec classes that can be used
        for compression/decompression.
    </description>
  </property>

  <property>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
    <description>The implementation for lzo codec.</description>
  </property>

<!-- file system properties -->

  <property>
    <name>fs.default.name</name>
    <!-- cluster variant -->
    <value>hdfs://<%= p("hadoop.namenode.host") %>:<%= p("hadoop.namenode.port")  %></value>
    <description>
        The name of the default file system.  Either the
        literal string "local" or a host:port for NDFS.
    </description>
    <final>true</final>
  </property>

  <property>
     <name>hadoop.tmp.dir</name>
     <value><%= p("hadoop.namenode.hadoop_tmp_dir") %>/${user.name}</value>
  </property>

  <property>
    <name>fs.inmemory.size.mb</name>
    <value>256</value>
  </property>

  <property>
    <name>fs.trash.interval</name>
    <value>360</value>
    <description>
        Number of minutes between trash checkpoints.
        If zero, the trash feature is disabled.
    </description>
  </property>

  <property>
    <name>fs.checkpoint.dir</name>
    <value>/var/vcap/data/hadoop/ssn</value>
    <description>
        Determines where on the local filesystem the DFS secondary
        name node should store the temporary images to merge.
        If this is a comma-delimited list of directories then the image is
        replicated in all of the directories for redundancy.
    </description>
  </property>

  <property>
    <name>fs.checkpoint.edits.dir</name>
    <value>${fs.checkpoint.dir}</value>
    <description>
        Determines where on the local filesystem the DFS secondary
        name node should store the temporary edits to merge.
        If this is a comma-delimited list of directoires then teh edits is
        replicated in all of the directoires for redundancy.
        Default value is same as fs.checkpoint.dir
    </description>
  </property>

  <property>
    <name>fs.checkpoint.period</name>
    <value>86400</value>
    <description>The number of seconds between two periodic checkpoints.</description>
  </property>

  <property>
    <name>fs.checkpoint.size</name>
    <value>2048000000</value>
    <description>
       The size of the current edit log (in bytes) that triggers a periodic
       checkpoint even if the fs.checkpoint.period hasn't expired.
    </description>
  </property>

  <property>
    <name>ipc.client.idlethreshold</name>
    <value>8000</value>
    <description>
        Defines the threshold number of connections after which
        connections will be inspected for idleness.
    </description>
  </property>

  <property>
    <name>ipc.client.connection.maxidletime</name>
    <value>30000</value>
    <description>
        The maximum time after which a client will bring down the
        connection to the server.
    </description>
  </property>

  <property>
    <name>ipc.client.connect.max.retries</name>
    <value>50</value>
    <description>Defines the maximum number of retries for IPC connections.</description>
  </property>

  <!-- Web Interface Configuration -->
  <property>
    <name>webinterface.private.actions</name>
    <value>false</value>
    <description>
        If set to true, the web interfaces of JT and NN may contain
        actions, such as kill job, delete file, etc., that should
        not be exposed to public. Enable this option if the interfaces
        are only reachable by those who have the right authorization.
    </description>
  </property>

  <property>
    <name>hadoop.security.authentication</name>
    <value>simple</value>
    <description>
        Set the authentication for the cluster. Valid values are: simple or kerberos.
    </description>
  </property>

  <property>
    <name>hadoop.security.authorization</name>
    <value>false</value>
    <description>Enable authorization for different protocols.</description>
  </property>

  <property>
    <name>hadoop.security.auth_to_local</name>
    <value>
        RULE:[2:$1@$0]([jt]t@.*EXAMPLE.COM)s/.*/mapred/
        RULE:[2:$1@$0]([nd]n@.*EXAMPLE.COM)s/.*/hdfs/
        RULE:[2:$1@$0](hm@.*EXAMPLE.COM)s/.*/hbase/
        RULE:[2:$1@$0](rs@.*EXAMPLE.COM)s/.*/hbase/
        DEFAULT</value>
<description>The mapping from kerberos principal names to local OS user names.
  So the default rule is just "DEFAULT" which takes all principals in your default domain to their first component.
  "omalley@APACHE.ORG" and "omalley/admin@APACHE.ORG" to "omalley", if your default domain is APACHE.ORG.
The translations rules have 3 sections:
      base     filter    substitution
The base consists of a number that represents the number of components in the principal name excluding the realm and the pattern for building the name from the sections of the principal name. The base uses $0 to mean the realm, $1 to mean the first component and $2 to mean the second component.

[1:$1@$0] translates "omalley@APACHE.ORG" to "omalley@APACHE.ORG"
[2:$1] translates "omalley/admin@APACHE.ORG" to "omalley"
[2:$1%$2] translates "omalley/admin@APACHE.ORG" to "omalley%admin"

The filter is a regex in parens that must the generated string for the rule to apply.

"(.*%admin)" will take any string that ends in "%admin"
"(.*@ACME.COM)" will take any string that ends in "@ACME.COM"

Finally, the substitution is a sed rule to translate a regex into a fixed string.

"s/@ACME\.COM//" removes the first instance of "@ACME.COM".
"s/@[A-Z]*\.COM//" removes the first instance of "@" followed by a name followed by ".COM".
"s/X/Y/g" replaces all of the "X" in the name with "Y"

So, if your default realm was APACHE.ORG, but you also wanted to take all principals from ACME.COM that had a single component "joe@ACME.COM", you'd do:

RULE:[1:$1@$0](.@ACME.ORG)s/@.//
DEFAULT

To also translate the names with a second component, you'd make the rules:

RULE:[1:$1@$0](.@ACME.ORG)s/@.//
RULE:[2:$1@$0](.@ACME.ORG)s/@.//
DEFAULT

If you want to treat all principals from APACHE.ORG with /admin as "admin", your rules would look like:

RULE[2:$1%$2@$0](.%admin@APACHE.ORG)s/./admin/
DEFAULT
    </description>
  </property>

  <property>
    <name>hadoop.proxyuser.hive.groups</name>
    <value>*</value>
    <description>Proxy group for Hadoop.</description>
  </property>

  <property>
    <name>hadoop.proxyuser.hive.hosts</name>
    <value>*</value>
    <description>Proxy host for Hadoop.</description>
  </property>

  <property>
    <name>hadoop.proxyuser.oozie.groups</name>
    <value>*</value>
    <description>Proxy group for Hadoop.</description>
  </property>

  <property>
    <name>hadoop.proxyuser.oozie.hosts</name>
    <value>*</value>
    <description>Proxy host for Hadoop.</description>
  </property>

  <property>
    <name>hadoop.proxyuser.templeton.groups</name>
    <value>users</value>
    <description>Proxy group for Hadoop.</description>
  </property>

  <property>
    <name>hadoop.proxyuser.templeton.hosts</name>
    <value>*</value>
    <description>Proxy host for Hadoop.</description>
  </property>

  <property>
    <name>hadoop.proxyuser.HTTP.groups</name>
    <value>users</value>
    <description>Proxy group for Hadoop.</description>
  </property>

  <property>
    <name>hadoop.proxyuser.HTTP.hosts</name>
    <value>*</value>
    <description>Proxy host for Hadoop.</description>
  </property><?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

<!-- file system properties -->

  <property>
    <name>dfs.name.dir</name>
    <!-- cluster variant -->
    <value>/opt/app/data01/nn,/opt/data/share01/nn</value>
    <description>
        Determines where on the local filesystem the DFS name node
        should store the name table.  If this is a comma-delimited list
        of directories then the name table is replicated in all of the
        directories, for redundancy.
    </description>
    <final>true</final>
  </property>

  <property>
    <name>dfs.support.append</name>
    <value>true</value>
    <description>to enable dfs append</description>
    <final>true</final>
  </property>

  <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
    <description>to enable webhdfs</description>
    <final>true</final>
  </property>

  <property>
    <name>dfs.datanode.failed.volumes.tolerated</name>
    <value>0</value>
    <description>#of failed disks dn would tolerate</description>
    <final>true</final>
  </property>

  <property>
    <name>dfs.block.local-path-access.user</name>
    <value>hbase</value>
    <description>the user who is allowed to perform short circuit reads.</description>
    <final>true</final>
  </property>

  <property>
    <name>dfs.data.dir</name>
    <value>/opt/data/data01/hdfs/dn,/opt/data/data02/hdfs/dn,/opt/data/data03/hdfs/dn,/opt/data/data04/hdfs/dn,/opt/data/data05/hdfs/dn,/opt/data/data06/hdfs/dn,/opt/data/data07/hdfs/dn,/opt/data/data08/hdfs/dn,/opt/data/data09/hdfs/dn,/opt/data/data10/hdfs/dn,/opt/data/data11/hdfs/dn,/opt/data/data12/hdfs/dn</value>
    <description>
        Determines where on the local filesystem an DFS data node
        should store its blocks.  If this is a comma-delimited
        list of directories, then data will be stored in all named
        directories, typically on different devices.
        Directories that do not exist are ignored.
    </description>
    <final>true</final>
  </property>

  <property>
    <name>dfs.hosts.exclude</name>
    <value>/opt/app/hdp-core/etc/hadoop/conf/dfs.exclude</value>
    <description>
        Names a file that contains a list of hosts that are not
        permitted to connect to the namenode.  The full pathname
        of the file must be specified.  If the value is empty,
        no hosts are excluded.
    </description>
  </property>

  <property>
    <name>dfs.hosts</name>
    <value>/opt/app/hdp-core/etc/hadoop/conf/dfs.include</value>
    <description>
        Names a file that contains a list of hosts that are permitted
        to connect to the namenode. The full pathname of the file must
        be specified.  If the value is empty, all hosts are permitted.
    </description>
  </property>

  <property>
    <name>dfs.replication.max</name>
    <value>50</value>
    <description>Maximal block replication.</description>
  </property>

  <property>
    <name>dfs.heartbeat.interval</name>
    <value>3</value>
    <description>Determines datanode heartbeat interval in seconds.</description>
  </property>

  <property>
    <name>dfs.safemode.threshold.pct</name>
    <value>1.0f</value>
    <description>
        Specifies the percentage of blocks that should satisfy
        the minimal replication requirement defined by dfs.replication.min.
        Values less than or equal to 0 mean not to start in safe mode.
        Values greater than 1 will make safe mode permanent.
        </description>
  </property>

  <property>
    <name>dfs.balance.bandwidthPerSec</name>
    <value>6250000</value>
    <description>
        Specifies the maximum amount of bandwidth that each datanode
        can utilize for the balancing purpose in term of
        the number of bytes per second.
  </description>
  </property>

  <property>
    <name>dfs.datanode.address</name>
    <value>0.0.0.0:50010</value>
  </property>

  <property>
    <name>dfs.datanode.http.address</name>
    <value>0.0.0.0:50075</value>
  </property>

  <property>
    <name>dfs.block.size</name>
    <value>134217728</value>
    <description>The default block size for new files.</description>
  </property>

  <property>
    <name>dfs.http.address</name>
    <value><%= p("hadoop.namenode.host") %>:50070</value>
    <description>
        The name of the default file system.  Either the
        literal string "local" or a host:port for NDFS.
    </description>
    <final>true</final>
  </property>

  <property>
    <name>dfs.datanode.du.reserved</name>
    <!-- cluster variant -->
    <value>1073741824</value>
    <description>
        Reserved space in bytes per volume.
        Always leave this much space free for non dfs use.
    </description>
  </property>

  <property>
    <name>dfs.datanode.ipc.address</name>
    <value>0.0.0.0:8010</value>
    <description>
        The datanode ipc server address and port.
        If the port is 0 then the server will start on a free port.
    </description>
  </property>

  <property>
    <name>dfs.blockreport.initialDelay</name>
    <value>120</value>
    <description>Delay for first block report in seconds.</description>
  </property>

  <property>
    <name>dfs.datanode.du.pct</name>
    <value>0.85f</value>
    <description>
        When calculating remaining space, only use
        this percentage of the real available space
    </description>
  </property>

  <property>
    <name>dfs.namenode.handler.count</name>
    <value>40</value>
    <description>The number of server threads for the namenode.</description>
  </property>

  <property>
    <name>dfs.datanode.max.xcievers</name>
    <value>1024</value>
    <description>PRIVATE CONFIG VARIABLE</description>
  </property>

  <!-- Permissions configuration -->

  <property>
    <name>dfs.umaskmode</name>
    <value>077</value>
    <description>The octal umask used when creating files and directories.</description>
  </property>

  <property>
    <name>dfs.web.ugi</name>
    <!-- cluster variant -->
    <value>gopher,gopher</value>
    <description>
        The user account used by the web interface.
        Syntax: USERNAME,GROUP1,GROUP2, ...
    </description>
  </property>

  <property>
    <name>dfs.permissions</name>
    <value>true</value>
    <description>
        If "true", enable permission checking in HDFS.
        If "false", permission checking is turned off,
        but all other behavior is unchanged.
        Switching from one parameter value to the other does not change the mode,
        owner or group of files or directories.
    </description>
  </property>

  <property>
    <name>dfs.permissions.supergroup</name>
    <value>hdfs</value>
    <description>The name of the group of super-users.</description>
  </property>

  <property>
    <name>dfs.namenode.handler.count</name>
    <value>100</value>
    <description>Added to grow Queue size so that more client connections are allowed</description>
  </property>

  <property>
    <name>ipc.server.max.response.size</name>
    <value>5242880</value>
  </property>

  <property>
    <name>dfs.block.access.token.enable</name>
    <value>true</value>
    <description>
        If "true", access tokens are used as capabilities for accessing datanodes.
        If "false", no access tokens are checked on accessing datanodes.
    </description>
  </property>

  <property>
    <name>dfs.namenode.kerberos.principal</name>
    <value>nn/_HOST@EXAMPLE.COM</value>
    <description>Kerberos principal name for the NameNode</description>
  </property>

  <property>
    <name>dfs.secondary.namenode.kerberos.principal</name>
    <value>nn/_HOST@EXAMPLE.COM</value>
    <description>Kerberos principal name for the secondary NameNode.</description>
  </property>


  <property>
    <!-- cluster variant -->
    <name>dfs.secondary.http.address</name>
    <value><%= p("hadoop.secondarynamenode.host") %>:50090</value>
    <description>Address of secondary namenode web server</description>
  </property>

  <property>
    <name>dfs.secondary.https.port</name>
    <value>50490</value>
    <description>The https port where secondary-namenode binds</description>
  </property>

  <property>
    <name>dfs.web.authentication.kerberos.principal</name>
    <value>HTTP/_HOST@EXAMPLE.COM</value>
    <description>
        The HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint.
        The HTTP Kerberos principal MUST start with 'HTTP/' per Kerberos
        HTTP SPNEGO specification.
    </description>
  </property>

  <property>
    <name>dfs.web.authentication.kerberos.keytab</name>
    <value>/etc/security/keytabs/spnego.service.keytab</value>
    <description>
        The Kerberos keytab file with the credentials for the
        HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint.
    </description>
  </property>

  <property>
    <name>dfs.datanode.kerberos.principal</name>
    <value>dn/_HOST@EXAMPLE.COM</value>
    <description>
        The Kerberos principal that the DataNode runs as. "_HOST" is replaced by the real host name.
    </description>
  </property>

  <property>
    <name>dfs.namenode.keytab.file</name>
    <value>/etc/security/keytabs/nn.service.keytab</value>
    <description>
        Combined keytab file containing the namenode service and host principals.
    </description>
  </property>

  <property>
    <name>dfs.secondary.namenode.keytab.file</name>
    <value>/etc/security/keytabs/nn.service.keytab</value>
    <description>
        Combined keytab file containing the namenode service and host principals.
    </description>
  </property>

  <property>
    <name>dfs.datanode.keytab.file</name>
    <value>/etc/security/keytabs/dn.service.keytab</value>
    <description>
        The filename of the keytab file for the DataNode.
    </description>
  </property>

  <property>
    <name>dfs.https.port</name>
    <value>50470</value>
 <description>The https port where namenode binds</description>

  </property>

  <property>
    <name>dfs.https.address</name>
    <value><%= p("hadoop.namenode.host") %>:50470</value>
    <description>The https address where namenode binds</description>
  </property>

  <property>
    <name>dfs.datanode.data.dir.perm</name>
    <value>750</value>
    <description>
        The permissions that should be there on dfs.data.dir
        directories. The datanode will not come up if the permissions are
        different on existing dfs.data.dir directories. If the directories
        don't exist, they will be created with this permission.
    </description>
  </property>

  <property>
    <name>dfs.access.time.precision</name>
    <value>0</value>
    <description>
        The access time for HDFS file is precise upto this value.
        The default value is 1 hour. Setting a value of 0 disables
        access times for HDFS.
    </description>
  </property>

  <property>
    <name>dfs.cluster.administrators</name>
    <value> hdfs</value>
    <description>ACL for who all can view the default servlets in the HDFS</description>
  </property>

  <property>
    <name>ipc.server.read.threadpool.size</name>
    <value>5</value>
    <description></description>
  </property>

  <property>
    <name>dfs.namenode.kerberos.internal.spnego.principal</name>
    <value>${dfs.web.authentication.kerberos.principal}</value>
  </property>

  <property>
    <name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name>
    <value>${dfs.web.authentication.kerberos.principal}</value>
  </property>
</configuration>